
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/07bMagda_lsuv.ipynb

from exp.nb_06cMagda import *

class Learner():
    def __init__(self, data_bunch, model, optimizer, loss_func, callback_list=[]):
        assert isinstance(data_bunch, DataBunch), 'First paramtere should be a DataBunch'
        assert isinstance(callback_list, list), 'Second paramters should be a LIST (of callbacks)'
        self.data_bunch = data_bunch
        self.model = model
        self.loss_func = loss_func
        self._get_optimizer(optimizer)
        self.callback_list = callback_list + [PlotsCallback()]
        for cb in self.callback_list:
            cb.init_learner(self)
        self.epoch = 0
        self.batch_x = None
        self.batch_y = None
        self.metrics = {}

    def _get_optimizer(self, optimizer, lr=0.5):
        self.optimizer = optimizer(self.model.parameters(), lr=lr)

    def forward(self, bx, by):
        logits = self.model(bx)
        loss = self.loss_func(logits, by)
        return logits, loss

    def backward(self, loss):
        loss.backward()
        self.callback('before_optim_step')
        self.optimizer.step()
        self.optimizer.zero_grad()

    def fit(self, epochs):
        assert isinstance(epochs, int), 'Number of epochs should be INT'
        self.total_epochs = epochs
        self.callback('fit_begin')
        for self.epoch in range(epochs):
            try:
                self.model.train()
                self.callback('train_begin')
                for (self.batch_x, self.batch_y) in self.data_bunch.train_dl:
                    self.callback('train_batch_begin')
                    logits, loss = self.forward(self.batch_x, self.batch_y)
                    self.backward(loss)
                    self.callback('train_batch_end', loss)
                self.model.eval()
                with torch.no_grad():
                    self.callback('validation_begin')
                    for (self.batch_x, self.batch_y) in self.data_bunch.valid_dl:
                        self.callback('validation_batch_begin')
                        logits, loss = self.forward(self.batch_x, self.batch_y)
                        self.callback('validation_batch_end', logits, loss)
                self.callback('epoch_end')
            except EarlyStopping:
                print(self.epoch, 'stopping ... ')
                break
        self.callback('fit_end')

    def callback(self, cb_name, *args, **kwargs):
        for cb in self.callback_list:
            cb_method = getattr(cb, cb_name, None)
            if cb_method:
                cb_method(*args, **kwargs)

class WeightsMonitoringCallback(Callback):
    def fit_begin(self):
        for layer in self.learner.model.children():
            if hasattr(layer, 'weight'):
                layer.weights_mean = []
                layer.weights_std = []
                layer.weights_mean = []
                layer.weights_std = []

    def before_optim_step(self):
        for layer in self.learner.model.children():
            try:
                weight = layer.weight.detach()
                layer.weights_mean.append(weight.mean().item())
                layer.weights_std.append(weight.std().item())
                layer.weights_mean.append(weight.grad.mean().item())
                layer.weights_std.append(weight.grad.std().item())
            except AttributeError:
                pass

class InitCallback(Callback):
    def fit_begin(self):
        for layer in self.learner.model.children():
            try:
                init.kaiming_normal_(layer.weight)
                init.zeros_(layer.bias)
            except AttributeError:
                pass
        print('All layers initialised')

def get_one_batch_bunch(data_bunch):
    batch_x, batch_y = data_bunch.train_data[0:data_bunch.train_bs]
    return batch_x, batch_y

def lsuv_init(model, batch_x):
    hooks = []

    for layer in model.children():
        try:
            init.kaiming_normal_(layer.weight)
            init.zeros_(layer.bias)
            hooks.append(layer.register_forward_hook(OutputStatsHook.forward_hook))
        except AttributeError:
                pass

    for i in range(10):
        model(batch_x)
        layers_done = []
        for layer in model.children():
            try:
                if abs(layer.output_stats['means'][-1]) > 1e-3:
                    layer.bias.data = layer.bias.data - layer.output_stats['means'][-1]
                    layers_done.append(False)
                else:
                    layers_done.append(True)
                if abs(layer.output_stats['stds'][-1] - 1) > 1e-3:
                    layer.weight.data = layer.weight.data / layer.output_stats['stds'][-1]
                    layers_done.append(False)
                else:
                    layers_done.append(True)
            except AttributeError:
                pass
        if all(layers_done):
            print('All layers initialised')
            break

    while len(hooks) > 0:
        hook = hooks.pop()
        hook.remove()
