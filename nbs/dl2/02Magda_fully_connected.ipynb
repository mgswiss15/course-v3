{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forwarda nd bakward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "\n",
    "* normalize\n",
    "* test near zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_01Magda import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/home/magda/datasets/mnist')\n",
    "data_gzip = data_path/'mnist.pkl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(data_gzip, 'rb') as data_file:\n",
    "    (train_x, train_y), (valid_x, valid_y), _ = pickle.load(data_file, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y, valid_x, valid_y) = [torch.tensor(x, dtype=torch.float) for x in (train_x, train_y, valid_x, valid_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y, valid_y = [torch.unsqueeze(x, -1) for x in (train_y, valid_y)]\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**normalize**\n",
    "across instances and dimensions cause all pixels treated equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(a, mean_a, std_a):\n",
    "    return (a - mean_a) / std_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_x.mean()\n",
    "train_std = train_x.std()\n",
    "\n",
    "train_x = normalize(train_x, train_mean, train_std)\n",
    "valid_x = normalize(valid_x, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-6.2598e-06), tensor(1.), tensor(-0.0059), tensor(0.9924))"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.mean(), train_x.std(), valid_x.mean(), valid_x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic archtecture\n",
    "\n",
    "* simplified kaiming init\n",
    "* def simple linear layer\n",
    "* def simple relu\n",
    "* kaiming init for relu\n",
    "* pytorch init kaiming\n",
    "* model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_num, in_dim = train_x.shape\n",
    "out_dim = 1\n",
    "hidden_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = torch.normal(0, 1, size=(in_dim, hidden_dim)) / in_dim**0.5\n",
    "bias1 = torch.zeros(hidden_dim)\n",
    "weights2 = torch.normal(0, 1, size=(hidden_dim, out_dim)) / hidden_dim**0.5\n",
    "bias1 = torch.zeros(out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-6.6535e-05), tensor(0.0359), tensor(1.0063))"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1.mean(), weights1.std(), weights1.std()*(in_dim**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_layer(x, w, b):\n",
    "    return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = lin_layer(valid_x, weights1, bias1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0611), tensor(0.9905))"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.mean(), hidden.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x.clamp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = relu(lin_layer(valid_x, weights1, bias1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4241), tensor(0.5941))"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.mean(), hidden.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**improved kaimining for relu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = torch.normal(0, 1, size=(in_dim, hidden_dim)) / (in_dim/2)**0.5\n",
    "bias1 = torch.zeros(hidden_dim)\n",
    "weights2 = torch.normal(0, 1, size=(hidden_dim, out_dim)) / (hidden_dim/2)**0.5\n",
    "bias2 = torch.zeros(out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = relu(lin_layer(valid_x, weights1, bias1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4999), tensor(0.7810))"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.mean(), hidden.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.empty((in_dim, hidden_dim))\n",
    "init.kaiming_normal_(w, mode='fan_out'); # pytorch w is weights1.T so fan_out instead of fan_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5579), tensor(0.8559))"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = relu(lin_layer(valid_x, w, bias1))\n",
    "hidden.mean(), hidden.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, w1, b1, w2, b2):\n",
    "    h = relu(lin_layer(x, w1, b1))\n",
    "    return lin_layer(h, w2, b2).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(valid_x, weights1, bias1, weights2, bias2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss funciton: MSE\n",
    "\n",
    "* model outputs\n",
    "* def mse loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(valid_x, weights1, bias1, weights2, bias2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(predict, target):\n",
    "    return ((predict-target)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36.4607)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mse(out, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradients and backward pass\n",
    "\n",
    "* def mse grad\n",
    "* def relu grad\n",
    "* def lin grad\n",
    "* def forwarda and backward as func\n",
    "* check results with pytorch autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(inp, target):\n",
    "    inp.g = 2*(inp - target) / inp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(inp, out):\n",
    "    inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, w, b, out):\n",
    "    inp.g = out.g@w.T\n",
    "    w.g = (inp[..., None]*out.g[:,None,:]).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_grad(out, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(x, w1, b1, w2, b2):\n",
    "    # forward\n",
    "    l1 = lin_layer(valid_x, weights1, bias1)\n",
    "    l1_relu = relu(l1)\n",
    "    out = lin_layer(l1_relu, weights2, bias2)\n",
    "    # mse\n",
    "    mse = loss_mse(out, valid_y)\n",
    "    # backward\n",
    "    mse_grad(out, valid_y)\n",
    "    lin_grad(l1_relu, weights2, bias2, out)\n",
    "    relu_grad(l1, l1_relu)\n",
    "    lin_grad(valid_x, weights1, bias1, l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_backward(valid_x, weights1, bias1, weights2, bias2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pytorch autograd**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        ...,\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1.requires_grad_(True)\n",
    "bias1.requires_grad_(True)\n",
    "weights2.requires_grad_(True)\n",
    "bias2.requires_grad_(True)\n",
    "valid_x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w1, b1, w2, b2):\n",
    "    # forward\n",
    "    l1 = lin_layer(valid_x, weights1, bias1)\n",
    "    l1_relu = relu(l1)\n",
    "    out = lin_layer(l1_relu, weights2, bias2)\n",
    "    # mse\n",
    "    return loss_mse(out, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = forward(valid_x, weights1, bias1, weights2, bias2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(weights1.grad, weights1.g, 1e-3, 1e-5)\n",
    "test_near(weights2.grad, weights2.g)\n",
    "test_near(bias1.grad, bias1.g)\n",
    "test_near(bias2.grad, bias2.g)\n",
    "test_near(valid_x.grad, valid_x.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refactor model - layers as classes\n",
    "\n",
    "* classes relu, lin, mse, model\n",
    "* `__call__`, backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp(0)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinLayer():\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp@self.w + self.b\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g@self.w.T\n",
    "        self.w.g = (self.inp[..., None]*self.out.g[:,None,:]).sum(0)\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE():\n",
    "    def __call__(self, inp, target):\n",
    "        self.inp = inp\n",
    "        self.target = target\n",
    "        return ((inp - target)**2).mean(0)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.inp.g = 2*(self.inp-self.target) / self.inp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.lin1 = LinLayer(w1, b1)\n",
    "        self.lin2 = LinLayer(w2, b2)\n",
    "        self.relu = Relu()\n",
    "        self.loss_func = LossMSE()\n",
    "        \n",
    "    def __call__(self, inp, target):\n",
    "        self.inp = inp\n",
    "        self.target = target\n",
    "        self.out = self.lin2(self.relu(self.lin1(inp)))\n",
    "        self.loss_func(self.out, self.target)\n",
    "        return self.out\n",
    "        \n",
    "    def loss(self):\n",
    "        return self.loss_func(self.out, self.target) \n",
    "\n",
    "    def backward(self):\n",
    "        self.loss_func.backward()\n",
    "        self.lin2.backward()\n",
    "        self.relu.backward()\n",
    "        self.lin1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1.g, bias1.g, weights2.g, bias2.g, valid_x.g = [None]*5\n",
    "model = Model(weights1, bias1, weights2, bias2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2343],\n",
       "        [-0.4456],\n",
       "        [-0.3779],\n",
       "        ...,\n",
       "        [ 0.4303],\n",
       "        [-0.4290],\n",
       "        [-0.9752]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36.7830], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**check agains pytorch autograd**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(weights1.grad, weights1.g, 1e-3, 1e-5)\n",
    "test_near(weights2.grad, weights2.g)\n",
    "test_near(bias1.grad, bias1.g)\n",
    "test_near(bias2.grad, bias2.g)\n",
    "test_near(valid_x.grad, valid_x.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refactor again with forward and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*self.args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self):\n",
    "        raise Exception('not implemented')\n",
    "    \n",
    "    def backward(self):\n",
    "        raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):   \n",
    "    def forward(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = self.inp.clamp(0)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinLayer(Module):\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = self.inp@self.w + self.b\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g@self.w.T\n",
    "        self.w.g = (self.inp[..., None]*self.out.g[:,None,:]).sum(0)\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Module):\n",
    "    def forward(self, inp, target):\n",
    "        self.inp = inp\n",
    "        self.target = target\n",
    "        return ((inp - target)**2).mean(0)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.inp.g = 2*(self.inp-self.target) / self.inp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [LinLayer(w1, b1), Relu(), LinLayer(w2, b2)]\n",
    "        self.loss_func = LossMSE()\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.loss = self.loss_func(x, target)\n",
    "        return x\n",
    "\n",
    "    def backward(self):\n",
    "        self.loss_func.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1.g, bias1.g, weights2.g, bias2.g, valid_x.g = [None]*5\n",
    "model = Model(weights1, bias1, weights2, bias2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2343],\n",
       "        [-0.4456],\n",
       "        [-0.3779],\n",
       "        ...,\n",
       "        [ 0.4303],\n",
       "        [-0.4290],\n",
       "        [-0.9752]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(weights1.grad, weights1.g, 1e-3, 1e-5)\n",
    "test_near(weights2.grad, weights2.g)\n",
    "test_near(bias1.grad, bias1.g)\n",
    "test_near(bias2.grad, bias2.g)\n",
    "test_near(valid_x.grad, valid_x.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear and nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layers = [nn.Linear(in_dim, h_dim), nn.ReLU(), nn.Linear(h_dim, out_dim)]\n",
    "        self.loss_func = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.loss = self.loss_func(x, target)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(in_dim, hidden_dim, out_dim)\n",
    "model(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1762, -0.1762, -0.1762,  ..., -0.1762, -0.1762, -0.1762],\n",
       "         [-0.0181, -0.0181, -0.0181,  ..., -0.0181, -0.0181, -0.0181],\n",
       "         [ 0.0960,  0.0960,  0.0960,  ...,  0.0960,  0.0960,  0.0960],\n",
       "         ...,\n",
       "         [ 0.0488,  0.0488,  0.0488,  ...,  0.0488,  0.0488,  0.0488],\n",
       "         [ 0.0147,  0.0147,  0.0147,  ...,  0.0147,  0.0147,  0.0147],\n",
       "         [-0.0384, -0.0384, -0.0384,  ..., -0.0384, -0.0384, -0.0384]]),\n",
       " tensor([ 0.4151,  0.0427, -0.2261,  0.3407, -0.1978, -0.0524, -0.0128, -0.2895,\n",
       "          0.0924, -0.1274, -0.2024,  0.4173,  0.0247, -0.5304, -0.1451, -0.5526,\n",
       "         -0.5436,  0.0343,  0.0569, -0.1933,  0.1652,  0.4987, -0.1080,  0.1236,\n",
       "         -0.0157,  0.0608, -0.3137,  0.0937, -0.0279,  0.0283,  0.0470,  0.1348,\n",
       "         -0.0759, -0.0946, -0.2766, -0.0417, -0.0631,  0.4763, -0.7488,  0.3939,\n",
       "         -0.5718, -0.4260,  0.1591,  0.1749, -0.1195,  0.1956, -0.2956, -0.1149,\n",
       "         -0.0346,  0.0904]))"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weight.grad, model.layers[0].bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.8590, -0.3108, -1.1811, -1.6412, -1.5475, -0.9389, -0.1614, -2.8028,\n",
       "          -0.8290, -1.2885, -0.7883, -4.3612, -0.5352, -1.5133, -3.1240, -1.7928,\n",
       "          -3.4015, -0.7169, -2.2674, -0.5153, -2.1296, -2.0411, -2.3259, -0.3523,\n",
       "          -2.0078, -1.4588, -1.9275, -2.0649, -0.9860, -1.3555, -0.3945, -0.3820,\n",
       "          -1.7299, -2.9450, -1.1033, -2.0713, -0.2237, -2.8358, -5.1573, -1.6168,\n",
       "          -1.8701, -1.5214, -0.4071, -0.5935, -0.2614, -3.6520, -1.0018, -1.8520,\n",
       "          -0.1577, -1.7664]]),\n",
       " tensor([-8.4678]))"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].weight.grad, model.layers[2].bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass updates the parameter gradients but the parameters cannot be directly extracted from the model as `model.parameters()` becuase they are included in the list in the `__init__`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
